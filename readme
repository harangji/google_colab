https://bit.ly/hg-01-3 : 최근접 이웃 알고리즘 58p
https://bit.ly/hg-02-1 : 훈련 세트와 테스트 세트, 넘파이 81p
https://bit.ly/hg-02-2 : 데이터 전처리, 사이킷런으로 훈련 세트/ 테스트 세트 나누기 (분할) 106p
https://bit.ly/hg-03-1 : k 최근접 이웃 회귀, k값 바꾸기 (k란? 가장 가까운 k개의 이웃), 평균 절댓값 오차 계산 124p
https://bit.ly/hg-03-2 : 선형 회귀(1차 방정식), 다항 선형 회귀(2차 방정식) 143p
https://bit.ly/hg-03-3 : 다중회귀, 새로운 특성 만들기, 스케일 지정, 릿지, 라쏘, alpha값 찾기 167p

로지스틱 회귀 중간고사에 나옴
머신러닝은 특성에 좌지우지되지만 딥러닝은 뉴런의 입력으로 특성이 뭔지 몰라도 되므로 가중치(기울기와 절편)로 학습한다.
k최근접 이웃 알고리즘은 머신러닝이 아니다. 학습하지 않기 때문이다.
훈련이란 각 노드들의 가중치를 구하는 것이다.
정답을 알려주고 반복학습을 통해 공식을 만든다. = 에포크 (반복횟수)
지도학습 : 정답을 알려주고 훈련
비지도학습 : 정답을 안알려주고 훈련
훈련 데이터 : 훈련 세트에는 질문,정답이 함께 들어가 있다.
샘플링 편향이 발생하지 않도록 전처리해야한다.
회귀란 평균에 수렴하는 것이다. 회귀는 분류와 다르게 값을 답으로 낼 수 있다. 값이 1에 가까울 수록 정확하다.
하이퍼파라미터 : 머신러닝훈련할때 조정해야하는 값
로지스틱 회귀 : 가장 기초적인 형태의 신경망, 분류에 사용함

활성 함수 : 인공 신경망(ANN) 및 딥러닝 모델에서 뉴런의 출력을 결정하는 함수입니다. 이 함수는 입력 신호의 가중합을 입력으로 받아 뉴런의 활성화 상태를 결정하며, 다음 레이어의 뉴런으로 신호를 전달하는 역할을 합니다. 
활성 함수는 신경망의 비선형성을 도입하여 신경망이 복잡한 함수를 모델링할 수 있도록 돕습니다.

시그모이드 함수 (Sigmoid Function): 시그모이드 함수는 입력을 0과 1 사이의 값으로 변환합니다. 이 함수는 이진 분류 문제에서 출력층에서 주로 사용되며, 
입력이 얼마나 높거나 낮은지를 나타내는 확률 값으로 해석할 수 있습니다. 그러나 경사 소실 문제로 인해 깊은 신경망에서는 사용이 제한됩니다.

하이퍼볼릭 탄젠트 함수 (Hyperbolic Tangent Function, Tanh): 하이퍼볼릭 탄젠트 함수는 입력을 -1과 1 사이의 값으로 변환합니다. 시그모이드 함수와 유사하지만 출력 범위가 더 넓어 중심이 0인 값으로 변환됩니다.

렐루 함수 (Rectified Linear Unit, ReLU): 렐루 함수는 입력이 양수인 경우에는 입력을 그대로 출력하고, 음수인 경우에는 0을 출력합니다.
이 함수는 신경망에서 가장 널리 사용되며, 계산이 간단하고 경사 하강법 학습을 가속화합니다. 그러나 입력이 음수인 경우, 뉴런은 활성화되지 않을 수 있어 "죽은 뉴런" 문제가 발생할 수 있습니다.

리키 렐루 함수 (Leaky ReLU): 리키 렐루 함수는 입력이 양수인 경우에는 입력을 그대로 출력하고, 음수인 경우에 작은 양수 값을 출력합니다. 
이렇게 함으로써 "죽은 뉴런" 문제를 완화하고 ReLU의 장점을 유지하려는 시도입니다.

소프트맥스 함수 (Softmax Function): 소프트맥스 함수는 다중 클래스 분류 문제에서 사용되며, 여러 출력 뉴런의 결과를 정규화하여 클래스 확률 분포를 생성합니다. 
모든 출력 합이 1이 되며 가장 큰 값을 갖는 클래스가 선택됩니다.

확률적 경사 하강법 : 바닥에 가까워질수록 정답에 근사한 값이 됨. 올라갔다 내려갔다 할수있게 기준값을 바꿈
배치 경사 하강법 : 경사 하강법은 모델을 학습시키는 최적화 알고리즘 중 하나로, 모델 파라미터를 업데이트하여 손실 함수를 최소화하는 방향으로 모델을 학습시킵니다.
손실 함수 : 정확도가 높은 것보다 손실률이 가장 낮은게 좋다.

머신러닝과 딥러닝 (머신러닝 안의 딥러닝)
딥러닝의 가장 기본적인 구성은 입력층과 출력층으로 구성되어있음
고양이 개 분류할때 => 출력층 2개

은닉층 : 입력층과 출력층 사이의 복합층이 2개 이상일 경우 hidden layer라고 부름. 히든 레이어가 너무 많으면 과대적합이 되어버림
심층 신경망은 hiddenlayer가 여러개일때를 말함

렐루 함수 : 이미지학습에 쓰임

옵티마이저 : 최적의 위치를 찾는 알고리즘
드롭아웃 : 인간의 필요하지 않은 기억을 버리는 능력을 모방. 뉴런 사이의 연결을 가늘다 못해 끊어지게 함. 쓸모없는 것을 동작하지 않게 함
합성곱 : 이미지를 학습할 때 패턴을 인식하기 위해 사용하는 필터. 도장을 만든다.
이미지는 나무를 가까이서 볼때와, 멀리 떨어져서 볼 수 있는 숲의 특성을 가짐
가시적과 거시적 특징이 합성곱으로 학습됨

